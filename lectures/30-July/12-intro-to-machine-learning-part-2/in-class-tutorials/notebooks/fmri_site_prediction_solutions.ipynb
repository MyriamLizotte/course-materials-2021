{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weighted-camcorder",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Predicting site from fMRI: cross-validation and dimensionality reduction.\n",
    "\n",
    "Here we consider the same data and prediction task as in the tutorial of\n",
    "Machine Learning part 1 of the QLS course.\n",
    "\n",
    "We have some fMRI time series, that we use to compute a connectivity matrix\n",
    "for each participant. We use the connectivity matrix values as our input\n",
    "features to predict to which site the participant belongs.\n",
    "\n",
    "As in Part 1, we classify participants using a logistic regression. However\n",
    "we make several additions.\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "We use scikit-learn's `sklearn.pipeline.Pipeline`, that enables chaining\n",
    "several transformations into a single scikit-learn estimator (an object with\n",
    "a `fit` method). This avoids dealing with the connectivity feature extraction\n",
    "separately and ensures everything is fitted on the training data only --\n",
    "which is crucial here because we will add a dimensionality reduction step\n",
    "with Principal Component Analysis.\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "We also consider a pipeline that reduces the dimension of input features with\n",
    "PCA, and compare it to the baseline logistic regrssion. One advantage is that\n",
    "the pipeline that uses PCA can be fitted much faster.\n",
    "\n",
    "## Cross-validation\n",
    "\n",
    "In part 1 we fitted one model and evaluated it on a held-out test set. Here,\n",
    "we will use scikit-learn's `cross_validate` to perform K-Fold\n",
    "cross-validation and get a better estimate of our model's generalization\n",
    "performance. This allows comparing logistic regression with and without PCA,\n",
    "as well as a naive baseline.\n",
    "\n",
    "Moreover, instead of the plain `LogisticRegression`, we use scikit-learn's\n",
    "`LogisticRegressionCV`, which automatically performs a nested\n",
    "cross-validation loop on the training data to select the best hyperparameter.\n",
    "\n",
    "We therefore obtain a typical supervised learning experiment, with learning\n",
    "pipelines that involve chained transformations, hyperparameter selection, a\n",
    "cross-validation, and comparison of several models and a baseline.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Read, understand and run this script. `load_connectivity_data` loads the data\n",
    "and returns the matrices `X` and `y`. `prepare_pipelines` returns a\n",
    "dictionary whose values are scikit-learn estimators and whose keys are names\n",
    "for each estimator. All estimators are instances of scikit-learn's\n",
    "`Pipeline`, and the first step is always connectivity feature extraction with\n",
    "nilearn's `ConnectivityMeasure`.\n",
    "\n",
    "At the moment `prepare_pipelines` only returns 2 estimators: the logistic\n",
    "regression and a dummy estimator. Add a third estimator in the returned\n",
    "dictionary, which contains a dimensionality reduction step: a PCA with 20\n",
    "components. To do so, add a `sklearn.decomposition.PCA` as the second step of\n",
    "the pipeline.\n",
    "\n",
    "There are 111 regions in the atlas we use to compute region-region\n",
    "connectivity matrices: the output of the `ConnectivityMeasure` has\n",
    "111 * (111 + 1) / 2 = 6216 columns. If the dataset has 100 participants, What\n",
    "is the size of the coefficients of the logistic regression? of the selected\n",
    "(20 first) principal components? of the output of the PCA transformation (ie\n",
    "the compressed design matrix)?\n",
    "Answer: 6216 coefficients + intercept; principal components: 20 x 6216;\n",
    "compressed X: 100 x 20.\n",
    "\n",
    "Here we are storing data and model coefficients in arrays of 64-bit\n",
    "floating-point values, meaning each number takes 64 bits = 8 bytes of memory.\n",
    "Approximately how much memory is used by the design matrix X? by the\n",
    "dimensionality-reduced data (ie the kept left singular vectors of X)? by the\n",
    "principal components (the kept right singular vectors of X)?\n",
    "Answer: X: 4,972,800 B, compressed X: 16,000 B, V: 994,560 B\n",
    "(+ 96 bytes for all for the array object)\n",
    "\n",
    "As you can see, in this script we do not specify explicitly the metric\n",
    "functions that are used to evaluate models, but rely on scikit-learn's\n",
    "defaults instead. What metric is used in order to select the best\n",
    "hyperparameter? What metric is used to compute scores in `cross_validate`?\n",
    "Are these defaults appropriate for our particular situation? Try replacing\n",
    "the default metrics with other scoring functions from scikit-learn or\n",
    "functions that you write yourself. Does the relative performance of the\n",
    "models change?\n",
    "Answer: sklearn.metrics.accuracy_score for both\n",
    "\n",
    "Add another estimator to the options returned by `prepare_pipelines`, that\n",
    "uses univariate feature selection instead of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "from nilearn.connectome import ConnectivityMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timeseries_and_site(n_subjects=100):\n",
    "    \"\"\"Load ABIDE timeseries and participants' site.\n",
    "\n",
    "    Returns X, a list with one array of shape (n_samples, n_rois) per\n",
    "    participant, and y, an array of length n_participants containing integers\n",
    "    representing the site each participant belongs to.\n",
    "\n",
    "    \"\"\"\n",
    "    data = datasets.fetch_abide_pcp(\n",
    "        n_subjects=n_subjects, derivatives=[\"rois_ho\"]\n",
    "    )\n",
    "    X = data[\"rois_ho\"]\n",
    "    y = preprocessing.LabelEncoder().fit_transform(\n",
    "        data[\"phenotypic\"][\"SITE_ID\"]\n",
    "    )\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pipelines():\n",
    "    \"\"\"Prepare scikit-learn pipelines for fmri classification with connectivity.\n",
    "\n",
    "    Returns a dictionary where each value is a scikit-learn estimator (a\n",
    "    `Pipeline`) and the corresponding key is a descriptive string for that\n",
    "    estimator.\n",
    "\n",
    "    As an exercise you need to add a pipeline that performs dimensionality\n",
    "    reduction with PCA.\n",
    "\n",
    "    \"\"\"\n",
    "    logistic_reg = Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"feature_extraction\",\n",
    "                ConnectivityMeasure(kind=\"correlation\", vectorize=True),\n",
    "            ),\n",
    "            (\"logistic\", LogisticRegressionCV(solver=\"liblinear\")),\n",
    "        ]\n",
    "    )\n",
    "    pca_logistic_reg = Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"feature_extraction\",\n",
    "                ConnectivityMeasure(kind=\"correlation\", vectorize=True),\n",
    "            ),\n",
    "            (\"pca\", PCA(n_components=20)),\n",
    "            (\"logistic\", LogisticRegressionCV(solver=\"liblinear\")),\n",
    "        ]\n",
    "    )\n",
    "    dummy = Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"feature_extraction\",\n",
    "                ConnectivityMeasure(kind=\"correlation\", vectorize=True),\n",
    "            ),\n",
    "            (\"classif\", DummyClassifier()),\n",
    "        ]\n",
    "    )\n",
    "    # TODO: add a pipeline with a PCA dimensionality reduction step to this\n",
    "    # dictionary. You will need to import `sklearn.decomposition.PCA`.\n",
    "    return {\n",
    "        \"Logistic no PCA\": logistic_reg,\n",
    "        \"Logistic with PCA\": pca_logistic_reg,\n",
    "        \"Dummy\": dummy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cv_scores(models, X, y):\n",
    "    \"\"\"Compute cross-validation scores for all models\n",
    "\n",
    "    `models` is a dictionary like the one returned by `prepare_pipelines`, ie\n",
    "    of the form `{\"model_name\": estimator}`, where `estimator` is a\n",
    "    scikit-learn estimator.\n",
    "\n",
    "    `X` and `y` are the design matrix and the outputs to predict.\n",
    "\n",
    "    Returns a `pd.DataFrame` with one row for each model and cross-validation\n",
    "    fold. Columns include `test_score` and `fit_time`.\n",
    "\n",
    "    \"\"\"\n",
    "    all_scores = []\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Computing scores for model: '{model_name}'\")\n",
    "        model_scores = pd.DataFrame(cross_validate(model, X, y))\n",
    "        model_scores[\"model\"] = model_name\n",
    "        all_scores.append(model_scores)\n",
    "    all_scores = pd.concat(all_scores)\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X, y = load_timeseries_and_site()\n",
    "    models = prepare_pipelines()\n",
    "    all_scores = compute_cv_scores(models, X, y)\n",
    "    print(all_scores.groupby(\"model\").mean())\n",
    "    sns.stripplot(data=all_scores, x=\"test_score\", y=\"model\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
