{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indie-responsibility",
   "metadata": {},
   "source": [
    "# Performing nested cross-validation\n",
    "\n",
    "Here we will evaluate the performance of an L2-regularized logistic\n",
    "regression on one of the classification datasets distributed with\n",
    "scikit-learn.\n",
    "\n",
    "The model has a hyperparameter C, which controls the regularization strength:\n",
    "a higher value of C means less regularization. We will automatically select\n",
    "the appropriate value for C among a grid of possible values with a nested\n",
    "cross-validation loop.\n",
    "\n",
    "The whole procedure therefore looks like:\n",
    "\n",
    "- Outer loop:\n",
    "  - obtain 5 (train, test) splits for the whole dataset\n",
    "  - initialize `scores` to an empty list\n",
    "  - for each (train, test) split:\n",
    "    + run grid-search (ie inner CV loop) on training data and obtain a model\n",
    "      fitted to the whole training data with the best hyperparameter.\n",
    "    + evaluate the model on the test data\n",
    "    + append the resulting score to `scores`\n",
    "  - return `scores`\n",
    "\n",
    "- Grid-search (inner loop):\n",
    "  - obtain 5 (train, test) splits for the available data (the training data\n",
    "    from the outer loop)\n",
    "  - initialize `hyperparameter_scores` to an empty list\n",
    "  - for each possible hyperparameter value C:\n",
    "    + initialize `cv_scores` to an empty list\n",
    "    + for each  train, test split:\n",
    "      * fit a model on train, using the hyperparameter C\n",
    "      * evaluate the model on test\n",
    "      * append the resulting score to `cv_scores`\n",
    "    + append the mean of `cv_scores` to `hyperparameter_scores`\n",
    "  - select the hyperparameter with the best mean score\n",
    "  - refit the model on the whole available data, using the selected\n",
    "    hyperparameter\n",
    "  - return this model\n",
    "\n",
    "Most of this logic is implemented in this module, but some key parts are\n",
    "still missing (marked with \"TODO\"). Your job is to complete the functions\n",
    "`cross_validate` and `grid_search` so that the whole nested cross-validation\n",
    "can be run.\n",
    "\n",
    "Some helper routines, `get_train_test_indices` and `fit_and_evaluate`, are\n",
    "provided to make the task easier. Make sure you read their code and\n",
    "understand what they do.\n",
    "\n",
    "The docstrings of the incomplete functions document precisely what are their\n",
    "parameters, and what they should compute and return. Rely on this information\n",
    "to write your implementation.\n",
    "\n",
    "This nested cross-validation procedure is often used, so scikit-learn\n",
    "provides all the functionality we are implementing here. To check that our\n",
    "implementation is correct, we can therefore compare results with what we\n",
    "obtain from scikit-learn. At the end of the file, you will see code that\n",
    "loads a scikit-learn dataset, and computes cross-validation scores using our\n",
    "`cross_validate` function, then using scikit-learn, and prints both results.\n",
    "If you execute this script by running `python nested_cross_validation.py`,\n",
    "these two results will be shown and you can check that the code runs and\n",
    "produces correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-statement",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import clone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-ranch",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Utilities\n",
    "\n",
    "The 2 functions below are helpers for the main routines `cross_validate` and\n",
    "`grid_search`. You should read them but they do not need to be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_indices(n_samples, k=5):\n",
    "    \"\"\"Given a total number of samples, return k-fold train, test indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "      The total number of samples in the dataset to be split for\n",
    "      cross-validation.\n",
    "\n",
    "    k : int, optional (default = 5)\n",
    "      The number of splits\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    splits : list[tuple[np.array[int], np.array[int]]]\n",
    "      each element of `splits` corresponds to one cross-validation fold and\n",
    "      contains a pair of arrays (train, test):\n",
    "      - train: the integer indices of samples in the training set\n",
    "      - test: the integer indices of samples in the testing set\n",
    "\n",
    "    \"\"\"\n",
    "    indices = np.arange(n_samples)\n",
    "    test_mask = np.empty(n_samples, dtype=bool)\n",
    "    splits = []\n",
    "    start = 0\n",
    "    for i in range(k):\n",
    "        n_test = n_samples // k\n",
    "        if i < n_samples % k:\n",
    "            n_test += 1\n",
    "        stop = start + n_test\n",
    "        test_mask[:] = False\n",
    "        test_mask[start:stop] = True\n",
    "        splits.append((indices[np.logical_not(test_mask)], indices[test_mask]))\n",
    "        start = stop\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-provision",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def fit_and_evaluate(model, C, X, y, train_idx, test_idx, score_func):\n",
    "    \"\"\"Fit a model on trainig data and compute its score on test data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learn estimator (will not be modified)\n",
    "      the estimator to be evaluated\n",
    "\n",
    "    C : float\n",
    "     The value for the regularization hyperparameter C to use when fitting the\n",
    "     model.\n",
    "\n",
    "    X : numpy array of shape (n_samples, n_features)\n",
    "      the full design matrix\n",
    "\n",
    "    y : numpy array of shape (n_samples, n_outputs) or (n_samples,)\n",
    "      the full target vector\n",
    "\n",
    "    train_idx : sequence of ints\n",
    "      the indices of training samples (row indices of X)\n",
    "\n",
    "    test_idx : sequence of ints\n",
    "      the indicies of testing samples\n",
    "\n",
    "    score_func : callable\n",
    "      the function that measures performance on test data, with signature\n",
    "     `score = score_func(true_y, predicted_y)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      The prediction score on test data\n",
    "\n",
    "    \"\"\"\n",
    "    model = clone(model)\n",
    "    model.set_params(C=C)\n",
    "    model.fit(X[train_idx], y[train_idx])\n",
    "    predictions = model.predict(X[test_idx])\n",
    "    score = score_func(y[test_idx], predictions)\n",
    "    print(\n",
    "        f\"    Inner CV loop: fit and evaluate one model; score = {score:.2f}\"\n",
    "    )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-stewart",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Exercises\n",
    "\n",
    "The two functions below are incomplete! Complete the body of each function so\n",
    "that it behaves as described in the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model, hyperparam_grid, X, y, score_func):\n",
    "    \"\"\"Inner loop of a nested cross-validation\n",
    "\n",
    "    This function estimates the performance of each hyperparameter in\n",
    "    `hyperparam_grid` with cross validation. It then selects the best\n",
    "    hyperparameter and refits a model on the whole data using the selected\n",
    "    hyperparameter. The fitted model is returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learn estimator\n",
    "      The base estimator, copies of which are trained and evaluated. `model`\n",
    "      itself is not modified.\n",
    "\n",
    "    hyperparam_grid : list[float]\n",
    "      list of possible values for the hyperparameter C.\n",
    "\n",
    "    X : numpy array of shape (n_samples, n_features)\n",
    "      the design matrix\n",
    "\n",
    "    y : numpy array of shape (n_samples, n_outputs) or (n_samples,)\n",
    "      the target vector\n",
    "\n",
    "    score_func : callable\n",
    "      the function computing the score on test data, with signature\n",
    "      `score = score_func(true_y, predicted_y)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_model : scikit-learn estimator\n",
    "      A copy of `model`, fitted on the whole `(X, y)` data, with the\n",
    "      (estimated) best hyperparameter.\n",
    "\n",
    "    \"\"\"\n",
    "    hyperparameter_scores = []\n",
    "    for C in hyperparam_grid:\n",
    "        print(f\"  Grid search: evaluate hyperparameter C = {C}\")\n",
    "        # **TODO** : run a cross-validation loop, using this particular\n",
    "        # hyperparameter C. Compute the mean of scores accross\n",
    "        # cross-validation folds and append it to\n",
    "        # `hyperparameter_scores`.\n",
    "        mean_score = \"TODO\"\n",
    "        hyperparameter_scores.append(mean_score)\n",
    "    # **TODO**: select the best hyperparameter according to the CV scores,\n",
    "    # refit the model on the whole data using this hyperparameter, and return\n",
    "    # the fitted model. Use `model.set_params` to set the hyperparameter\n",
    "    best_C = \"TODO\"\n",
    "    print(f\"  ** Grid search: keep best hyperparameter C = {best_C} **\")\n",
    "    # `clone` is to work with a copy of `model` instead of modifying the\n",
    "    # argument itself.\n",
    "    best_model = clone(model)\n",
    "    # ...\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model, hyperparam_grid, X, y, score_func, k=5):\n",
    "    \"\"\"\n",
    "    Get cross-validation score with an inner CV loop to select hyperparameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learn estimator, for example `LogisticRegression()`\n",
    "      The base model to fit and evaluate. `model` itself is not modified.\n",
    "\n",
    "    hyperparam_grid : list[float]\n",
    "      list of possible values for the hyperparameter C.\n",
    "\n",
    "    X : numpy array of shape (n_samples, n_features)\n",
    "      the design matrix\n",
    "\n",
    "    y : numpy array of shape (n_samples, n_outputs) or (n_samples,)\n",
    "      the target vector\n",
    "\n",
    "    score_func : callable\n",
    "      the function computing the score on test data, with signature\n",
    "      `score = score_func(true_y, predicted_y)`.\n",
    "\n",
    "    k : int, optional\n",
    "        the number of splits for the k-fold cross-validation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scores : list[float]\n",
    "       The scores obtained for each of the cross-validation folds\n",
    "\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for i, (train_idx, test_idx) in enumerate(\n",
    "        get_train_test_indices(len(y), k=k)\n",
    "    ):\n",
    "        print(f\"\\nOuter CV loop: fold {i}\")\n",
    "        # **TODO**: complete the cross-validation loop. For each train, test\n",
    "        # split, use `grid_search` to run an inner cross-validation loop on the\n",
    "        # train data, then evaluate the resulting model on test data and store\n",
    "        # the resulting score.\n",
    "        score = \"TODO\"\n",
    "        print(f\"Outer CV loop: finished fold {i}, score: {score:.2f}\")\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-favor",
   "metadata": {},
   "source": [
    "## Trying our routines on real data\n",
    "\n",
    "The code below gets executed when you run this script with `python\n",
    "nested_cross_validation`. It computes a cross-validation score with our code,\n",
    "and compares it to the results obtained with scikit-learn. Read it to see how\n",
    "what we have implemented would be easily done with scikit-learn.\n",
    "\n",
    "Here we have written this logic ourselves to understand how it works, but in\n",
    "practice in real projects we would use the scikit-learn functionality which\n",
    "is more flexible, more reliable and faster.\n",
    "\n",
    "Note: this code will only run once you have completed the exercises!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from sklearn import datasets, linear_model, model_selection, metrics\n",
    "\n",
    "    # X, y = datasets.load_wine(return_X_y=True)\n",
    "    X, y = datasets.load_iris(return_X_y=True)\n",
    "    idx = np.arange(len(y))\n",
    "    np.random.RandomState(0).shuffle(idx)\n",
    "    # this is now the recommended way of doing this, but only works with recent\n",
    "    # versions of numpy:\n",
    "    # np.random.default_rng(0).shuffle(idx)\n",
    "    X, y = X[idx], y[idx]\n",
    "    model = linear_model.LogisticRegression()\n",
    "    hyperparam_grid = [0.001, 0.01, 0.1]\n",
    "    score_func = metrics.accuracy_score\n",
    "    my_scores = cross_validate(model, hyperparam_grid, X, y, score_func)\n",
    "\n",
    "    grid_search_model = model_selection.GridSearchCV(\n",
    "        model,\n",
    "        {\"C\": hyperparam_grid},\n",
    "        scoring=\"accuracy\",\n",
    "        cv=model_selection.KFold(5),\n",
    "    )\n",
    "    sklearn_scores = model_selection.cross_validate(\n",
    "        grid_search_model,\n",
    "        X,\n",
    "        y,\n",
    "        scoring=\"accuracy\",\n",
    "        cv=model_selection.KFold(5),\n",
    "    )[\"test_score\"]\n",
    "    print(\"\\n\\nMy scores:\")\n",
    "    print(my_scores)\n",
    "    print(\"Scikit-learn scores:\")\n",
    "    print(list(sklearn_scores))\n",
    "    assert np.allclose(\n",
    "        my_scores, sklearn_scores\n",
    "    ), \"Results differ from scikit-learn!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-candidate",
   "metadata": {},
   "source": [
    "## Additional exercise (optional)\n",
    "\n",
    "Have you noticed the hyperparameter grid was specified slightly differently\n",
    "for the scikit-learn `GridSearchCV`? we passed a dictionary:\n",
    "`{\"C\": [0.001, # 0.01, 0.1 ]}`.\n",
    "\n",
    "This is because with `GridSearchCV` we can specify values for several\n",
    "hyperparameters, for example:\n",
    "`{\"C\": [0.001, 0.01, 0.1], \"penalty\": [\"l1\", # \"l2\"]}`,\n",
    "and all combinations of these will be tried.\n",
    "\n",
    "Modify this module so that we can specify such a hyperparameter grid, rather\n",
    "than only a list of values for a specific hyperparameter named \"C\". Hint:\n",
    "check the documentation for the `set_params` function of scikit-learn\n",
    "estimators. You may want to use the python dict unpacking syntax, for example\n",
    "`model.set_params(**hyperparams)`. You can also use `itertools.product` from\n",
    "the python standard library to easily build all the combinations of\n",
    "hyperparameters."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
