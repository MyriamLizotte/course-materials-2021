{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-cruise",
   "metadata": {},
   "source": [
    "Question: what is the issue in the code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=80, n_features=600, noise=10, random_state=0)\n",
    "\n",
    "model = Ridge(alpha=1e-8)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "mse = mean_squared_error(y, predictions)\n",
    "\n",
    "print(f\"\\nMean Squared Error: {mse}\")\n",
    "print(\"MSE is 0 up to machine precision:\", np.allclose(mse, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-ethics",
   "metadata": {},
   "source": [
    "Let's compare training and testing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(\n",
    "    model, X, y, return_train_score=True, scoring=\"neg_mean_squared_error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-impossible",
   "metadata": {},
   "source": [
    "Question: what CV strategy are we using? what would be the default `scoring`\n",
    "if we did not specify it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = -pd.DataFrame(scores)\n",
    "errors = (\n",
    "    errors.loc[:, [\"train_score\", \"test_score\"]]\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"train_score\": \"Training error\",\n",
    "            \"test_score\": \"Testing error\",\n",
    "        }\n",
    "    )\n",
    "    .stack()\n",
    "    .reset_index()\n",
    ")\n",
    "errors.columns = [\"split\", \"data\", \"score\"]\n",
    "\n",
    "sns.stripplot(data=errors, x=\"score\", y=\"data\")\n",
    "plt.gca().set_xlabel(\"\")\n",
    "plt.gca().set_ylabel(\"\")\n",
    "plt.gca().set_title(\"Mean Squared Error\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
