* export options                                                   :noexport:
** general
   #+STARTUP: beamer
   #+OPTIONS: H:3 toc:nil num:t date:nil

   #+LaTeX_CLASS: beamer
   #+LaTeX_CLASS_OPTIONS: [presentation,mathserif,table]

** presentation info
   #+TITLE: Machine learning Part 2
   # #+AUTHOR: Jérôme Dockès

   #+BEAMER_HEADER: \author{Jérôme Dockès}
   #+BEAMER_HEADER: \titlegraphic{\includegraphics[height=1.5cm]{figures/mcgill-university.png} \hspace{1.5cm} \includegraphics[height=1.5cm]{figures/origami-lab-logo.png}}
   #+BEAMER_HEADER: \date{QLS course 2021-07-30}

** latex headers
*** fonts and beamer
    #+LaTeX_HEADER: \beamertemplatenavigationsymbolsempty

    #+LaTeX_HEADER: \usepackage[T1]{fontenc}

    #+LaTeX_HEADER: \usepackage{DejaVuSans}
    # #+LaTeX_HEADER: \usepackage{DejaVuSansMono}

    # #+LaTeX_HEADER: \usepackage[default]{opensans}
    # #+LaTeX_HEADER: \usepackage{lmodern}
    # #+LaTeX_HEADER: \usepackage{libertine}
    # #+LaTeX_HEADER: \usepackage{iwona}
    # #+LaTeX_HEADER: \usepackage[sc,osf]{mathpazo}
    # #+LaTeX_HEADER: \usepackage{mathptmx}
    # #+LaTeX_HEADER: \usepackage{helvet}
    # #+LaTeX_HEADER: \usefonttheme{default}

    # #+LaTeX_HEADER: \usefonttheme{serif}
    #+LaTeX_HEADER: \usefonttheme{professionalfonts}

    #+LaTeX_HEADER: \usepackage[euler-digits,euler-hat-accent]{eulervm}

    # #+LaTeX_HEADER: \setbeamertemplate{itemize items}[circle]
    #+LaTeX_HEADER: \setbeamertemplate{itemize items}{•}
    #+LaTeX_HEADER: \setbeamertemplate{enumerate items}[default]

    # #+LaTex_HEADER: \AtBeginSection[]
    # #+LaTex_HEADER: {
    # #+LaTex_HEADER: \begin{frame}<beamer>
    # #+LaTex_HEADER: \frametitle{Outline}
    # #+LaTex_HEADER: \tableofcontents[currentsection]
    # #+LaTex_HEADER: \end{frame}
    # #+LaTex_HEADER: }
    # #+LaTex_HEADER: \setcounter{tocdepth}{1}

    #+LaTeX_HEADER: \setbeamertemplate{headline}{}
    #+LaTeX_HEADER: \setbeamertemplate{footline}{
    #+LaTeX_HEADER: \leavevmode%
    #+LaTeX_HEADER: \hbox{%
    #+LaTeX_HEADER: \begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,right]{fg=black}%
    #+LaTeX_HEADER:     \usebeamerfont{section in head/foot}\insertsection\hspace*{2em}
    #+LaTeX_HEADER:     \insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    #+LaTeX_HEADER: \end{beamercolorbox}%
    #+LaTeX_HEADER: }%
    #+LaTeX_HEADER: \vskip0pt%
    #+LaTeX_HEADER: }
    #+LaTeX_HEADER: \usepackage{appendixnumberbeamer}

    #+LaTeX_HEADER: \setbeamersize{text margin left=3mm,text margin right=3mm}
*** footnote citations
    #+LaTeX_HEADER: \newcommand\blfootnote[1]{%
    #+LaTeX_HEADER: \begingroup
    #+LaTeX_HEADER: \renewcommand\thefootnote{}\footnote{#1}%
    #+LaTeX_HEADER: \addtocounter{footnote}{-1}%
    #+LaTeX_HEADER:  \endgroup
    #+LaTeX_HEADER: }
    #+LaTeX_HEADER: \setbeamerfont{footnote}{size=\tiny}
*** other imports
    #+LaTeX_HEADER: \usepackage{tikz}
    #+LaTeX_HEADER: \usepackage[retainorgcmds]{IEEEtrantools}
    #+LaTeX_HEADER: \hypersetup{colorlinks=true, allcolors=., urlcolor=blue}
    #+LaTeX_HEADER: \usepackage[absolute,overlay]{textpos}
*** math operators
    #+LaTex_HEADER: \newcommand{\eg}{e.g.\,}
    #+LaTex_HEADER: \newcommand{\ie}{i.e.\,}
    #+LaTex_HEADER: \newcommand{\aka}{a.k.a.\,}
    #+LaTex_HEADER: \newcommand{\etc}{\emph{etc.}\,}

    #+LaTex_HEADER: \newcommand{\X}{{\mathbold X}}
    #+LaTex_HEADER: \newcommand{\bSigma}{{\mathbold \Sigma}}
    #+LaTex_HEADER: \newcommand{\x}{{\mathbold x}}
    #+LaTex_HEADER: \newcommand{\bbeta}{{\mathbold \beta}}
    #+LaTex_HEADER: \newcommand{\Y}{{\mathbold Y}}
    #+LaTex_HEADER: \newcommand{\y}{{\mathbold y}}
    #+LaTex_HEADER: \newcommand{\B}{{\mathbold B}}
    #+LaTex_HEADER: \newcommand{\W}{{\mathbold W}}
    #+LaTex_HEADER: \newcommand{\U}{{\mathbold U}}
    #+LaTex_HEADER: \newcommand{\V}{{\mathbold V}}
    #+LaTex_HEADER: \newcommand{\bH}{{\mathbold H}}
    #+LaTex_HEADER: \newcommand{\R}{\mathbb{R}}
    #+LaTex_HEADER: \DeclareMathOperator*{\argmin}{argmin}
    #+LaTex_HEADER: \DeclareMathOperator*{\argmax}{argmax}
    #+LaTex_HEADER: \DeclareMathOperator*{\tv}{TV}
    #+LaTex_HEADER: \DeclareMathOperator*{\Tr}{Tr}
    #+LaTex_HEADER: \DeclareMathOperator*{\FFT}{FFT}
    #+LaTex_HEADER: \DeclareMathOperator*{\IFFT}{IFFT}
    #+LaTex_HEADER: \DeclareMathOperator*{\diag}{diag}
    #+LaTex_HEADER: \DeclareMathOperator*{\supp}{supp}
    #+LaTex_HEADER: \DeclareMathOperator*{\tf}{tf}
    #+LaTex_HEADER: \DeclareMathOperator*{\idf}{idf}
    #+LaTex_HEADER: \DeclareMathOperator*{\df}{df}
    #+LaTex_HEADER: \DeclareMathOperator*{\Var}{Var}
    #+LaTex_HEADER: \DeclareMathOperator*{\Frob}{Frob}
    #+LaTex_HEADER: \DeclareMathOperator*{\F}{F}
    #+LaTex_HEADER: \DeclareMathOperator*{\softmax}{softmax}
    #+LaTex_HEADER: \DeclareMathOperator*{\AUC}{AUC}

    #+LaTeX_HEADER: \usepackage{bm}

** color theme
   # #+BEAMER_COLOR_THEME: dove
   # #+BEAMER_COLOR_THEME: seagull

   #+LaTeX_HEADER: \usecolortheme{dove}
   #+LaTeX_HEADER: \setbeamercolor*{block title example}{fg=black,bg=white}
   #+LaTeX_HEADER: \setbeamercolor*{block body example}{fg=black,bg=white}
* Intro
** Recap of part 1
*** Recap of part 1
**** Supervised learning
     :PROPERTIES:
     :BEAMER_act: <1->
     :END:
       - Regression: least-squares linear regression
       - Classification: logistic regression
**** Regularization
     :PROPERTIES:
     :BEAMER_act: <2->
     :END:
       - \(\ell_2\) \aka ridge regularization
**** Model evaluation and selection
     :PROPERTIES:
     :BEAMER_act: <3->
     :END:
       - Out-of-sample generalization; independent test set
       - Performance metrics:
         - regression: mean squared error
         - classification: accuracy, ROC curve
       - Cross-validation
****  
     :PROPERTIES:
     :BEAMER_act: <4->
     :END:
  Don't remember? watch Part 1 again!
*** Notation & vocabulary
**** Supervised learning framework
 \begin{equation}
 Y = f(X) + E
 \end{equation}
\vspace{-10pt}
#+ATTR_BEAMER: :overlay +-
 - \(Y \in \R \): output (\aka target, dependent variable) to predict
 - \(X \in \R^p \): features (\aka inputs, regressors, descriptors, independent variables)
 - \(E \in \R \): unmodelled noise
 - \(f\): the function we try to approximate
**** Example: linear regression
     :PROPERTIES:
     :BEAMER_act: <4->
     :END:
\vspace{-20pt}
 \begin{IEEEeqnarray}{rCl}
 Y & = & \beta_0 + \langle X, \beta \rangle + E \\
& = & \beta_0 + \sum_{j=1}^p X_j \, \beta_j + E
 \end{IEEEeqnarray}
"learning" = estimating \(\beta_0 \in \R\) and \(\beta \in \R^p\)
* Dimensionality reduction
** Intro
*** Dimensionality reduction
**** Until now
     #+ATTR_LATEX: :height .12 \textheight
     [[file:figures/graphs/pipeline-1.pdf]]
**** Add a step in the pipeline: simplifying the inputs
     #+ATTR_LATEX: :height .12 \textheight
     [[file:figures/graphs/pipeline-2.pdf]]
*** Dimensionality reduction
**** Problems when the number of features \(p\) becomes large
     - Bigger errors on test data (larger variance of predictions)
     - Numerical stability issues
     - Computational cost and memory usage
*** Toy example: linear regression with simulated data
    - Generate \(\X \in \R^{n \times 3}\), \(\mathbold{\bbeta} \in \R^3\), and \(\y = \X \, \bbeta \in R^n\)
    - Append columns containing random noise to \(\X\)
    - Now \(\X \in \R^{n \times p}\), with \(p \geq 3\), but only the first 3 columns are linked with \(\y\)
    - Split into training and testing tests and evaluate a linear regression model: what happens when \(p\) becomes large?
  \vfill

See [[https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression][=sklearn.datasets.make_regression=]] for generating data
*** Model complexity: overfitting
    - Model complexity increases with dimension.
    - Example: a linear model in dimension \(p\) can fit exactly (0 training error) any set of \(p + 1\) points.
    - Risk of overfitting: fitting exactly training data but failing on test data

    #+ATTR_LATEX: :height .7\textheight
    [[file:figures/generated/ridge_overfitting/mse.pdf]]
*** Cost of fitting many parameters
    - Many algorithms require polynomial time in \(p\)
    - Implementations often make copies of the design matrix (\eg for centering & rescaling)
    #+ATTR_LATEX: :height .7\textheight
    [[file:figures/generated/ridge_overfitting/durations.pdf]]
** Univariate feature selection
*** Univariate feature selection
    - \aka feature screening, filtering ...
    - Check features (columns of \(\X\)) one by one for association with the output \(\y\)
    - Keep only a fixed number or percentage of the features
**** Simple (linear) association criteria
     :PROPERTIES:
     :BEAMER_act: <2->
     :END:
     - for regression: correlation
     - for classification: ANalysis Of VAriance
**** Read more in the scikit-learn user guide
     :PROPERTIES:
     :BEAMER_act: <2->
     :END:
 https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection

*** Univariate feature selection
    Keeping only the 10 best features (most correlated with \(\y\))
    #+ATTR_LATEX: :height .7\textheight
    [[file:figures/generated/ridge_overfitting/mse_with_dim_reduction.pdf]]

*** Same plot in log scale
    #+ATTR_LATEX: :height .7\textheight
    [[file:figures/generated/ridge_overfitting/mse_with_dim_reduction_log.pdf]]

** Linear decomposition methods
*** Linear decomposition methods
**** Maybe OK to drop $\X_2$:
     \vspace{-10pt}
     #+ATTR_LATEX: :height .3\textheight
     [[file:figures/generated/pca/cloud_aligned.pdf]]
     \vspace{-20pt}
**** Data low-dimensional but no feature can be dropped:
     #+ATTR_LATEX: :height .3\textheight
     [[file:figures/generated/pca/cloud_not_aligned.pdf]]

Find a better referential in which to represent the data
*** COMMENT Linear regression: projection on the column space of \(\X\)
**** Approximate \(y\) as a combination of the columns of \(X\)
  \begin{equation}
  \hat{\y} = \X \, \hat{\bbeta} \in \R^n
  \end{equation}
- The columns of \(X\) are a family of \(p\) \(n\)-dimensional vectors
- When \(p\) is high or the columns of \(X\) are correlated, we want to use a family of \(k < p\) instead
- Feature selection: drop some columns, keep only \(k\)
- Could we build a better family of \(k\) vectors?
*** Linear regression: projection on the column space of \(X\)
**** top                                                     :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .3
      :END:
      \begin{equation}
      \hat{\y} = \X \, \hat{\bbeta}
      \end{equation}

***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .7
      :END:
      \vspace{-17pt}
      #+ATTR_LATEX: :height .7\textheight
      [[file:figures/generated/dim_reduction_colors/regression_full_3.pdf]]

**** bottom                                                  :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
     - Too many features: high variance & unstable solution
     - Feature selection: drop some columns of \(\X\)
     - Other ways to build a family of \(k\) vectors on which to regress \(\y\)?
*** Linear decomposition: low-rank approximation of \(\X\)
    Minimize
\begin{equation}
\| \X - \W \, \bH \|_{\F}^2 = \sum_{i, j} ( \X_{i,j} - (\W \, \bH)_{i,j})^2
\end{equation}
    #+ATTR_LATEX: :height .5\textheight
    [[file:figures/generated/dim_reduction_colors/factorization_3.pdf]]
*** Linear regression after dimensionality reduction
    \begin{equation}
    \hat{\y} = \W \, \hat{\bbeta}
    \end{equation}
    #+ATTR_LATEX: :height .7\textheight
    [[file:figures/generated/dim_reduction_colors/regression_reduced_3.pdf]]
*** Prediction for a new data point \(\x \in \R^{p}\)
    - Find the combination of rows of \(\bH\) that is closest to \(\x\): regress \(\x\) on \(\bH^T\)
    - Multiply by \(\hat{\bbeta}\)
    \begin{equation}
\x \in \R^p \rightarrow \text{projection} \rightarrow \mathbold{w} \in \R^k \rightarrow \langle \cdot \, , \, \hat{\bbeta}\rangle \rightarrow \hat{y} \in \R
    \end{equation}
*** Principal Component Analysis
    - Matrix factorization based on Singular Value Decomposition:
    \begin{equation}
    \X = \U \, \bSigma \, \V^T
    \end{equation}
    with \(\X \in \R^{n \times p}\), \(\U \in \R^{n \times p}\), \(\bSigma \in \R^{p \times p}\), \(\V \in \R^{p \times p}\)
    - \(\bSigma \succeq 0\) diagonal with decreasing values \(\bSigma_{j,j}\) along the diagonal
    - \(\U^T\, \U = I_p\)
    - \(\V^T\, \V = I_p\)

Truncating the SVD to keep only the first \(k\) components gives the best rank-\(k\) approximation of \(\X\)
#+ATTR_LATEX: :height .3\textheight
[[file:figures/generated/pca/cloud_not_aligned_with_pc.pdf]]

*** Other decomposition methods
Many other methods use the same objective (sum of squared reconstruction errors), but add penalties or constraints on the factors
- Dictionary Learning
- Non-negative Matrix Factorization
- K-means clustering
- ...

**** What about \(\y\)?
     - PCA is an example of /unsupervised/ learning: it does not use \(\y\)
     - Some other methods take it into account: \eg Partial Least Squares
*** Ridge regression and PCA
    - Both ridge regression and PC regression compute the coordinates of \(\y\) in the basis given by the SVD of \(\X\)
    - ridge shrinks the coefficients of Singular Vector \(j\) by a factor \(\sigma_j^2 / (\sigma_j^2 + \lambda)\)
    - PC regression sets the coefficient to 0 for all but the \(k\) largest \(\sigma_j\)

#+ATTR_LATEX: :height .6\textheight
[[file:figures/generated/dim_reduction_colors/regression_reduced_3_svd.pdf]]
* More on cross-validation
** More on cross-validation

*** Setting hyperparameters
**** How can we choose:
     - Number of features or PCA components \(k\)?
     - The ridge hyperparameter \(\lambda\)?
**** answer                                                 :B_ignoreheading:
     :PROPERTIES:
     :BEAMER_env: ignoreheading
     :END:
     Try a few and pick the best one...

     But measure its performance on separate data!
*** Need for fresh test data
    When you hear "best", "maximum", "select", ... think "bias"
- I have 4 dice and want to find one that rolls high numbers
- I roll them all once and select the die that gives the highest number
- The selected die rolled a 5. Is 5 a good estimate of that die's average result? What if I had 1,000 dice?
- I need to roll it again to get an unbiased estimate
# #+ATTR_LATEX: :height .5 \textheight
# [[file:figures/generated/regression_to_mean/dice_rolls.pdf]]
*** Accuracy of the best model
- Several models are trained, then evaluated on a separate test set
- All models give random answers -- expected accuracy is .5
- If I select the best one, its measured accuracy is biased ...
#+ATTR_LATEX: :height .5 \textheight
[[file:figures/generated/hyperparameter_overfitting/accuracies.pdf]]
*** Nested cross-validation
When you hear "best", "maximum", "select", ... think "bias"
**** Setting the parameters
    :PROPERTIES:
    :BEAMER_act: <2->
    :END:
     - *Select* \(\bbeta\) that gives the *best* prediction on training data
     - The prediction score for \(\hat{\bbeta}\) is biased: compute a new score on unseen test data.
**** Setting the hyperparameters
    :PROPERTIES:
    :BEAMER_act: <3->
    :END:
     - Repeat step 1 for a few values of \(\lambda\), \(k\), \etc., fitting and testing several models
     - *Select* the hyperparameter that obtains the *best* prediction on test data
     - The prediction score of that model on /test/ data is biased: evaluate it again on unseen data
*** One split
[[file:figures/generated/train_eval_test/datasets.pdf]]
*** Nested cross-validation
[[file:figures/generated/train_eval_test/cv.pdf]]
see  [[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html][=sklearn.model_selection.GridSearchCV=]]
*** Some common pitfalls with cross-validation
**** Fitting part of the pipeline on the whole dataset
     :PROPERTIES:
     :BEAMER_act: <1->
     :END:
       + \eg fit PCA on all data, then do cross-validation on dim-reduced dataset
       + use  [[https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html][=sklearn.pipeline.Pipeline=]]
**** Ignoring dependencies between samples
     :PROPERTIES:
     :BEAMER_act: <2->
     :END:
         + Multiple datapoints per participant
         + Time series
**** Ignoring dependencies between CV scores
     :PROPERTIES:
     :BEAMER_act: <3->
     :END:
         + Training sets overlap: cross-validation scores of different splits are not independent
**** Over-interpreting good CV scores
     :PROPERTIES:
     :BEAMER_act: <4->
     :END:
         + Good CV scores on one dataset do not mean the model will always perform well on a new dataset

*** COMMENT Two sources of variance: training data and test sample
    Don't use Leave-One-Out Cross-validation
* Neuroimaging application: a task-fMRI decoding pipeline
** FMRI decoding
*** Supervised learning with fMRI
    - Predict in which site / with which scanner a resting-state fMRI sequence was acquired
*** The decoding pipeline
    - Masking: extracting voxels that are inside the brain
    - Connectivity: measuring correlations between brain regions to build a feature vector for each participant
    - Univariate feature selection with ANalysis Of VAriance
    - Classifier: logistic regression
*** Implementation: in class
