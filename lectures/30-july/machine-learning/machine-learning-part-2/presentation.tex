% Created 2021-07-12 Mon 15:04
% Intended LaTeX compiler: pdflatex
\documentclass[presentation,mathserif,table]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\beamertemplatenavigationsymbolsempty
\usepackage[T1]{fontenc}
\usepackage{DejaVuSans}
\usefonttheme{professionalfonts}
\usepackage[euler-digits,euler-hat-accent]{eulervm}
\setbeamertemplate{itemize items}{•}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{headline}{}
\setbeamertemplate{footline}{
\leavevmode%
\hbox{%
\begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,right]{fg=black}%
\usebeamerfont{section in head/foot}\insertsection\hspace*{2em}
\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
\end{beamercolorbox}%
}%
\vskip0pt%
}
\usepackage{appendixnumberbeamer}
\setbeamersize{text margin left=3mm,text margin right=3mm}
\newcommand\blfootnote[1]{%
\begingroup
\renewcommand\thefootnote{}\footnote{#1}%
\addtocounter{footnote}{-1}%
\endgroup
}
\setbeamerfont{footnote}{size=\tiny}
\usepackage{tikz}
\usepackage[retainorgcmds]{IEEEtrantools}
\hypersetup{colorlinks=true, allcolors=., urlcolor=blue}
\usepackage[absolute,overlay]{textpos}
\newcommand{\eg}{e.g.\,}
\newcommand{\ie}{i.e.\,}
\newcommand{\aka}{a.k.a.\,}
\newcommand{\etc}{\emph{etc.}\,}
\newcommand{\X}{{\mathbold X}}
\newcommand{\bSigma}{{\mathbold \Sigma}}
\newcommand{\x}{{\mathbold x}}
\newcommand{\bbeta}{{\mathbold \beta}}
\newcommand{\Y}{{\mathbold Y}}
\newcommand{\y}{{\mathbold y}}
\newcommand{\B}{{\mathbold B}}
\newcommand{\W}{{\mathbold W}}
\newcommand{\U}{{\mathbold U}}
\newcommand{\V}{{\mathbold V}}
\newcommand{\bH}{{\mathbold H}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\tv}{TV}
\DeclareMathOperator*{\Tr}{Tr}
\DeclareMathOperator*{\FFT}{FFT}
\DeclareMathOperator*{\IFFT}{IFFT}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\tf}{tf}
\DeclareMathOperator*{\idf}{idf}
\DeclareMathOperator*{\df}{df}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\Frob}{Frob}
\DeclareMathOperator*{\F}{F}
\DeclareMathOperator*{\softmax}{softmax}
\DeclareMathOperator*{\AUC}{AUC}
\usepackage{bm}
\usecolortheme{dove}
\setbeamercolor*{block title example}{fg=black,bg=white}
\setbeamercolor*{block body example}{fg=black,bg=white}
\usetheme{default}
\author{Jerome Dockes}
\date{}
\title{Machine learning Part 2}
\author{Jérôme Dockès}
\titlegraphic{\includegraphics[height=1.5cm]{figures/mcgill-university.png} \hspace{1.5cm} \includegraphics[height=1.5cm]{figures/origami-lab-logo.png}}
\date{QLS course 2021-07-30}
\hypersetup{
 pdfauthor={Jerome Dockes},
 pdftitle={Machine learning Part 2},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.3.7)}, 
 pdflang={English}}
\begin{document}

\maketitle
\section{Intro}
\label{sec:orga13c162}
\subsection{Recap of part 1}
\label{sec:org3f6e514}
\begin{frame}[label={sec:org17f5100}]{Recap of part 1}
\begin{block}{Supervised learning}
\begin{itemize}
\item Regression: least-squares linear regression
\item Classification: logistic regression
\end{itemize}
\end{block}
\begin{block}{Regularization}
\begin{itemize}
\item \(\ell_2\) \aka ridge regularization
\end{itemize}
\end{block}
\begin{block}{Model evaluation and selection}
\begin{itemize}
\item Out-of-sample generalization; independent test set
\item Performance metrics:
\begin{itemize}
\item regression: mean squared error
\item classification: accuracy, ROC curve
\end{itemize}
\item Cross-validation
\end{itemize}
\end{block}
\begin{structureenv} %% part1
Don't remember? watch Part 1 again!
\end{structureenv}
\end{frame}
\begin{frame}[label={sec:org059f4c1}]{Notation \& vocabulary}
\begin{block}{Supervised learning framework}
\begin{equation}
Y = f(X) + E
\end{equation}
\vspace{-10pt}
\begin{itemize}
\item \(Y \in \R\): output (\aka target, dependent variable) to predict
\item \(X \in \R^p\): features (\aka inputs, regressors, descriptors, independent variables)
\item \(E \in \R\): unmodelled noise
\item \(f\): the function we try to approximate
\end{itemize}
\end{block}
\begin{block}{Example: linear regression}
\vspace{-20pt}
 \begin{IEEEeqnarray}{rCl}
 Y & = & \beta_0 + \langle X, \beta \rangle + E \\
& = & \beta_0 + \sum_{j=1}^p X_j \, \beta_j + E
 \end{IEEEeqnarray}
"learning" = estimating \(\beta_0 \in \R\) and \(\beta \in \R^p\)
\end{block}
\end{frame}
\section{Dimensionality reduction}
\label{sec:org621a6d0}
\subsection{Intro}
\label{sec:orgb42431d}
\begin{frame}[label={sec:orgaccec9a}]{Dimensionality reduction}
\begin{block}{Until now}
\begin{center}
\includegraphics[height=.12 \textheight]{figures/graphs/pipeline-1.pdf}
\end{center}
\end{block}
\begin{block}{Add a step in the pipeline: simplifying the inputs}
\begin{center}
\includegraphics[height=.12 \textheight]{figures/graphs/pipeline-2.pdf}
\end{center}
\end{block}
\end{frame}
\begin{frame}[label={sec:org6be13f0}]{Dimensionality reduction}
\begin{block}{Problems when the number of features \(p\) becomes large}
\begin{itemize}
\item Bigger errors on test data (larger variance of predictions)
\item Numerical stability issues
\item Computational cost and memory usage
\end{itemize}
\end{block}
\end{frame}
\begin{frame}[label={sec:orga6a7be0},fragile]{Toy example: linear regression with simulated data}
 \begin{itemize}
\item Generate \(\X \in \R^{n \times 3}\), \(\mathbold{\bbeta} \in \R^3\), and \(\y = \X \, \bbeta \in R^n\)
\item Append columns containing random noise to \(\X\)
\item Now \(\X \in \R^{n \times p}\), with \(p \geq 3\), but only the first 3 columns are linked with \(\y\)
\item Split into training and testing tests and evaluate a linear regression model: what happens when \(p\) becomes large?
\end{itemize}
\vfill

See \href{https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make\_regression.html\#sklearn.datasets.make\_regression}{\texttt{sklearn.datasets.make\_regression}} for generating data
\end{frame}
\begin{frame}[label={sec:org41d9d84}]{Model complexity: overfitting}
\begin{itemize}
\item Model complexity increases with dimension.
\item Example: a linear model in dimension \(p\) can fit exactly (0 training error) any set of \(p + 1\) points.
\item Risk of overfitting: fitting exactly training data but failing on test data
\end{itemize}

\begin{center}
\includegraphics[height=.7\textheight]{figures/generated/ridge_overfitting/mse.pdf}
\end{center}
\end{frame}
\begin{frame}[label={sec:orgf365c26}]{Cost of fitting many parameters}
\begin{itemize}
\item Many algorithms require polynomial time in \(p\)
\item Implementations often make copies of the design matrix (\eg for centering \& rescaling)
\end{itemize}
\begin{center}
\includegraphics[height=.7\textheight]{figures/generated/ridge_overfitting/durations.pdf}
\end{center}
\end{frame}
\subsection{Univariate feature selection}
\label{sec:org985704a}
\begin{frame}[label={sec:org122101e}]{Univariate feature selection}
\begin{itemize}
\item \aka feature screening, filtering \ldots{}
\item Check features (columns of \(\X\)) one by one for association with the output \(\y\)
\item Keep only a fixed number or percentage of the features
\end{itemize}
\begin{block}{Simple (linear) association criteria}
\begin{itemize}
\item for regression: correlation
\item for classification: ANalysis Of VAriance
\end{itemize}
\end{block}
\begin{block}{Read more in the scikit-learn user guide}
\url{https://scikit-learn.org/stable/modules/feature\_selection.html\#feature-selection}
\end{block}
\end{frame}

\begin{frame}[label={sec:orgf2eb1b0}]{Univariate feature selection}
Keeping only the 10 best features (most correlated with \(\y\))
\begin{center}
\includegraphics[height=.7\textheight]{figures/generated/ridge_overfitting/mse_with_dim_reduction.pdf}
\end{center}
\end{frame}

\begin{frame}[label={sec:orgc9ce5ec}]{Same plot in log scale}
\begin{center}
\includegraphics[height=.7\textheight]{figures/generated/ridge_overfitting/mse_with_dim_reduction_log.pdf}
\end{center}
\end{frame}

\subsection{Linear decomposition methods}
\label{sec:orgbbd61cb}
\begin{frame}[label={sec:org4a4d202}]{Linear decomposition methods}
\begin{block}{Maybe OK to drop \(\X_2\):}
\vspace{-10pt}
\begin{center}
\includegraphics[height=.3\textheight]{figures/generated/pca/cloud_aligned.pdf}
\end{center}
\vspace{-20pt}
\end{block}
\begin{block}{Data low-dimensional but no feature can be dropped:}
\begin{center}
\includegraphics[height=.3\textheight]{figures/generated/pca/cloud_not_aligned.pdf}
\end{center}

Find a better referential in which to represent the data
\end{block}
\end{frame}
\begin{frame}[label={sec:org390b610}]{Linear regression: projection on the column space of \(X\)}
\begin{structureenv} %% top
\begin{columns}
\begin{column}{.3\columnwidth}
\begin{equation}
\hat{\y} = \X \, \hat{\bbeta}
\end{equation}
\end{column}

\begin{column}{.7\columnwidth}
\vspace{-17pt}
\begin{center}
\includegraphics[height=.7\textheight]{figures/generated/dim_reduction_colors/regression_full_3.pdf}
\end{center}
\end{column}
\end{columns}
\end{structureenv}

\begin{structureenv} %% bottom
\begin{itemize}
\item Too many features: high variance \& unstable solution
\item Feature selection: drop some columns of \(\X\)
\item Other ways to build a family of \(k\) vectors on which to regress \(\y\)?
\end{itemize}
\end{structureenv}
\end{frame}
\begin{frame}[label={sec:org3c99a6f}]{Linear decomposition: low-rank approximation of \(\X\)}
Minimize
\begin{equation}
\| \X - \W \, \bH \|_{\F}^2 = \sum_{i, j} ( \X_{i,j} - (\W \, \bH)_{i,j})^2
\end{equation}
\begin{center}
\includegraphics[height=.5\textheight]{figures/generated/dim_reduction_colors/factorization_3.pdf}
\end{center}
\end{frame}
\begin{frame}[label={sec:orgb56f213}]{Linear regression after dimensionality reduction}
\begin{equation}
\hat{\y} = \W \, \hat{\bbeta}
\end{equation}
\begin{center}
\includegraphics[height=.7\textheight]{figures/generated/dim_reduction_colors/regression_reduced_3.pdf}
\end{center}
\end{frame}
\begin{frame}[label={sec:orgac330c6}]{Prediction for a new data point \(\x \in \R^{p}\)}
\begin{itemize}
\item Find the combination of rows of \(\bH\) that is closest to \(\x\): regress \(\x\) on \(\bH^T\)
\item Multiply by \(\hat{\bbeta}\)
\end{itemize}
    \begin{equation}
\x \in \R^p \rightarrow \text{projection} \rightarrow \mathbold{w} \in \R^k \rightarrow \langle \cdot \, , \, \hat{\bbeta}\rangle \rightarrow \hat{y} \in \R
    \end{equation}
\end{frame}
\begin{frame}[label={sec:org2386c05}]{Principal Component Analysis}
\begin{itemize}
\item Matrix factorization based on Singular Value Decomposition:
\end{itemize}
\begin{equation}
\X = \U \, \bSigma \, \V^T
\end{equation}
with \(\X \in \R^{n \times p}\), \(\U \in \R^{n \times p}\), \(\bSigma \in \R^{p \times p}\), \(\V \in \R^{p \times p}\)
\begin{itemize}
\item \(\bSigma \succeq 0\) diagonal with decreasing values \(\bSigma_{j,j}\) along the diagonal
\item \(\U^T\, \U = I_p\)
\item \(\V^T\, \V = I_p\)
\end{itemize}

Truncating the SVD to keep only the first \(k\) components gives the best rank-\(k\) approximation of \(\X\)
\begin{center}
\includegraphics[height=.3\textheight]{figures/generated/pca/cloud_not_aligned_with_pc.pdf}
\end{center}
\end{frame}

\begin{frame}[label={sec:orge968557}]{Other decomposition methods}
Many other methods use the same objective (sum of squared reconstruction errors), but add penalties or constraints on the factors
\begin{itemize}
\item Dictionary Learning
\item Non-negative Matrix Factorization
\item K-means clustering
\item \ldots{}
\end{itemize}

\begin{block}{What about \(\y\)?}
\begin{itemize}
\item PCA is an example of \emph{unsupervised} learning: it does not use \(\y\)
\item Some other methods take it into account: \eg Partial Least Squares
\end{itemize}
\end{block}
\end{frame}
\begin{frame}[label={sec:org6ee4f75}]{Ridge regression and PCA}
\begin{itemize}
\item Both ridge regression and PC regression compute the coordinates of \(\y\) in the basis given by the SVD of \(\X\)
\item ridge shrinks the coefficients of Singular Vector \(j\) by a factor \(\sigma_j^2 / (\sigma_j^2 + \lambda)\)
\item PC regression sets the coefficient to 0 for all but the \(k\) largest \(\sigma_j\)
\end{itemize}

\begin{center}
\includegraphics[height=.6\textheight]{figures/generated/dim_reduction_colors/regression_reduced_3_svd.pdf}
\end{center}
\end{frame}
\section{More on cross-validation}
\label{sec:orge357a9b}
\subsection{More on cross-validation}
\label{sec:org8aeb564}

\begin{frame}[label={sec:org8bf626b}]{Setting hyperparameters}
\begin{block}{How can we choose:}
\begin{itemize}
\item Number of features or PCA components \(k\)?
\item The ridge hyperparameter \(\lambda\)?
\end{itemize}
\end{block}
Try a few and pick the best one\ldots{}

But measure its performance on separate data!
\end{frame}
\begin{frame}[label={sec:org2b155b8}]{Need for fresh test data}
When you hear "best", "maximum", "select", \ldots{} think "bias"
\begin{itemize}
\item I have 4 dice and want to find one that rolls high numbers
\item I roll them all once and select the die that gives the highest number
\item The selected die rolled a 5. Is 5 a good estimate of that die's average result? What if I had 1,000 dice?
\item I need to roll it again to get an unbiased estimate
\end{itemize}
\end{frame}
\begin{frame}[label={sec:orgff64d20}]{Accuracy of the best model}
\begin{itemize}
\item Several models are trained, then evaluated on a separate test set
\item All models give random answers -- expected accuracy is .5
\item If I select the best one, its measured accuracy is biased \ldots{}
\end{itemize}
\begin{center}
\includegraphics[height=.5 \textheight]{figures/generated/hyperparameter_overfitting/accuracies.pdf}
\end{center}
\end{frame}
\begin{frame}[label={sec:orgc992771}]{Nested cross-validation}
When you hear "best", "maximum", "select", \ldots{} think "bias"
\begin{block}{Setting the parameters}
\begin{itemize}
\item \alert{Select} \(\bbeta\) that gives the \alert{best} prediction on training data
\item The prediction score for \(\hat{\bbeta}\) is biased: compute a new score on unseen test data.
\end{itemize}
\end{block}
\begin{block}{Setting the hyperparameters}
\begin{itemize}
\item Repeat step 1 for a few values of \(\lambda\), \(k\), \etc., fitting and testing several models
\item \alert{Select} the hyperparameter that obtains the \alert{best} prediction on test data
\item The prediction score of that model on \emph{test} data is biased: evaluate it again on unseen data
\end{itemize}
\end{block}
\end{frame}
\begin{frame}[label={sec:orge123826}]{One split}
\begin{center}
\includegraphics[width=.9\linewidth]{figures/generated/train_eval_test/datasets.pdf}
\end{center}
\end{frame}
\begin{frame}[label={sec:org59ef20d},fragile]{Nested cross-validation}
 \begin{center}
\includegraphics[width=.9\linewidth]{figures/generated/train_eval_test/cv.pdf}
\end{center}
see  \href{https://scikit-learn.org/stable/modules/generated/sklearn.model\_selection.GridSearchCV.html}{\texttt{sklearn.model\_selection.GridSearchCV}}
\end{frame}
\begin{frame}[label={sec:orgb47380a},fragile]{Some common pitfalls with cross-validation}
 \begin{itemize}
\item Fitting part of the pipeline on the whole dataset
\begin{itemize}
\item \eg fit PCA on all data, then do cross-validation on dim-reduced dataset
\item use  \href{https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html}{\texttt{sklearn.pipeline.Pipeline}}
\end{itemize}
\item Ignoring dependencies between samples
\begin{itemize}
\item Multiple datapoints per participant
\item Time series
\item \ldots{}
\end{itemize}
\item Ignoring dependencies between CV scores
\begin{itemize}
\item Training sets overlap: cross-validation scores of different splits are not independent
\end{itemize}
\item Over-interpreting good CV scores
\begin{itemize}
\item Good CV scores on one dataset do not mean the model will always perform well on a new dataset
\end{itemize}
\end{itemize}
\end{frame}

\section{Neuroimaging application: a task-fMRI decoding pipeline}
\label{sec:org59876a1}
\subsection{FMRI decoding}
\label{sec:orgd978e45}
\begin{frame}[label={sec:orge1a13d3}]{Supervised learning with fMRI}
\begin{itemize}
\item Predict in which site / with which scanner a resting-state fMRI sequence was acquired
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org87f32a1}]{The decoding pipeline}
\begin{itemize}
\item Masking: extracting voxels that are inside the brain
\item Connectivity: measuring correlations between brain regions to build a feature vector for each participant
\item Univariate feature selection with ANalysis Of VAriance
\item Classifier: logistic regression
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org83afe70}]{Implementation: in class}
\end{frame}
\end{document}